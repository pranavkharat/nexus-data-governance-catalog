# NEXUS GraphRAG: Knowledge Graph-Driven Data Catalog with GraphRAG 

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Neo4j](https://img.shields.io/badge/Neo4j-5.x-green.svg)](https://neo4j.com/)
[![Milvus](https://img.shields.io/badge/Milvus-2.x-00A1EA.svg)](https://milvus.io/)
[![Ollama](https://img.shields.io/badge/Ollama-llama3.1-orange.svg)](https://ollama.ai/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


---

## ğŸ“š Course Information

| Field | Details |
|-------|---------|
| **Course** | DAMG7374 17611 ST: LLM w/ Knowledge Graph DB |
| **Program** | Master of Science in Information Systems |
| **University** | Northeastern University |
| **Term** | Fall 2025 |
| **Student** | Pranav Kharat, Shreeanant Bharadwaj, Venkat Akash Varun |
| **Status** | âœ… Implementation Complete |

---

## ğŸ“‹ Table of Contents

- [Executive Summary](#-executive-summary)
- [Key Contributions](#-key-contributions)
- [System Architecture](#-system-architecture)
- [Research Questions & Results](#-research-questions--results)
- [Technology Stack](#-technology-stack)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Building the Knowledge Graph](#-building-the-knowledge-graph)
- [Running the Demo](#-running-the-demo)
- [Project Structure](#-project-structure)
- [Knowledge Graph Schema](#-knowledge-graph-schema)
- [Datasets](#-datasets)
- [Query Types & Examples](#-query-types--examples)
- [Evaluation Methodology](#-evaluation-methodology)
- [API Reference](#-api-reference)
- [Known Limitations](#-known-limitations)
- [Future Work](#-future-work)
- [References](#-references)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## ğŸ¯ Executive Summary

**NEXUS** (Next-generation Enterprise eXploration and Unified Search) is a Knowledge Graph-driven Data Catalog with GraphRAG capabilities. It extends the [SANTOS](https://dl.acm.org/doi/10.1145/3588689) research (SIGMOD 2023) from a table union search prototype into a production-ready enterprise data governance system.

### The Problem

Modern enterprises struggle with:
- **Data Silos**: Metadata scattered across Snowflake, Databricks, and other platforms
- **Discovery Challenges**: Finding relevant tables requires tribal knowledge
- **Lineage Gaps**: Understanding data flow requires manual documentation
- **Governance Overhead**: Ensuring data quality and compliance is labor-intensive
- **Duplicate Detection**: Identifying redundant data across platforms is difficult

### The Solution

NEXUS provides a unified semantic layer that:
1. **Federates metadata** from multiple platforms into a single knowledge graph
2. **Enables natural language querying** through hybrid GraphRAG
3. **Automatically extracts lineage** from query history
4. **Detects duplicates** across platforms using SANTOS-inspired algorithms
5. **Validates governance constraints** using SHACL-inspired rules
6. **Explains matches** with human-readable natural language

### Key Finding

**Rule-based query routing outperforms machine learning on small structured datasets** â€” achieving 60% accuracy vs. XGBoost's 53.3%. This challenges the assumption that ML always beats heuristics and has implications for enterprise metadata systems where training data is limited.

---

## ğŸ† Key Contributions

| # | Contribution | Impact |
|---|--------------|--------|
| 1 | **Hybrid GraphRAG** outperforming embeddings-only RAG | +20% accuracy improvement |
| 2 | **Rule-based routing** beating ML on small datasets | 60% vs 53.3% (6.7pp improvement) |
| 3 | **Automated lineage extraction** from Snowflake | 100% F1 score |
| 4 | **SHACL-inspired governance** framework | 10 constraint shapes, 3 severity levels |
| 5 | **Privacy-preserving federation** across platforms | Metadata-only, no raw data transfer |
| 6 | **Cross-platform SANTOS** duplicate detection | 16 cross-source matches detected |
| 7 | **Multi-source Text-to-Cypher** with query-specific prompts | 100% intent classification |
| 8 | **Explainable matching** with WHY explanations | Natural language reasoning |

---

## ğŸ— System Architecture

### Five-Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LAYER 5: PRESENTATION                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                     Gradio Demo Interface                         â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Unified â”‚ Lineage â”‚ Compare â”‚Duplicateâ”‚  Perf   â”‚Governance â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ Search  â”‚Explorer â”‚ Engines â”‚Detectionâ”‚Benchmarkâ”‚  SHACL    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                         LAYER 4: LLM LAYER                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  Ollama llama3.1 (7B parameters)                  â”‚  â”‚
â”‚  â”‚         Local inference â€¢ No API costs â€¢ Privacy-preserving       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      LAYER 3: INTELLIGENCE                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Smart     â”‚   Learned   â”‚  Unified LLM â”‚    Explainable       â”‚   â”‚
â”‚  â”‚  GraphRAG   â”‚  (XGBoost)  â”‚   Router     â”‚     GraphRAG         â”‚   â”‚
â”‚  â”‚  (60% acc)  â”‚  (53.3%)    â”‚ (100% intent)â”‚  (WHY explanations)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LangChain  â”‚   Lineage   â”‚    SHACL     â”‚   Cross-Source       â”‚   â”‚
â”‚  â”‚ Text-to-    â”‚  Extractor  â”‚  Validator   â”‚    Detector          â”‚   â”‚
â”‚  â”‚   Cypher    â”‚             â”‚              â”‚ (SANTOS-adapted)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        LAYER 2: STORAGE                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚     Neo4j Knowledge Graph      â”‚     Milvus Vector Database     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ 393+ nodes             â”‚  â”‚  â”‚ â€¢ 15 table embeddings    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ 996+ relationships     â”‚  â”‚  â”‚ â€¢ 384-dimensional vectorsâ”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ 4-layer structure      â”‚  â”‚  â”‚ â€¢ HNSW index             â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ 9 relationship types   â”‚  â”‚  â”‚ â€¢ Cosine similarity      â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       LAYER 1: DATA SOURCES                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        â„ï¸ Snowflake            â”‚         ğŸ§± Databricks          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ TRAINING_DB            â”‚  â”‚  â”‚ â€¢ workspace.sample_data  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ 13 Olist tables        â”‚  â”‚  â”‚ â€¢ 2 tables (250 rows)    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ 1.4M rows total        â”‚  â”‚  â”‚ â€¢ Unity Catalog          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ 3 schemas              â”‚  â”‚  â”‚ â€¢ SQL Warehouse          â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Routing Architecture

```
                              User Query
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    UnifiedLLMGraphRAG       â”‚
                    â”‚    classify_query_intent()  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼           â–¼           â–¼           â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚cross_    â”‚â”‚databricksâ”‚â”‚ metadata â”‚â”‚sample_   â”‚â”‚sensitivityâ”‚
    â”‚source    â”‚â”‚          â”‚â”‚          â”‚â”‚data      â”‚â”‚          â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚           â”‚           â”‚           â”‚           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        LangChain Text-to-Cypher     â”‚
                    â”‚  â€¢ Query-type-specific prompts      â”‚
                    â”‚  â€¢ 38 few-shot examples             â”‚
                    â”‚  â€¢ Schema-aware generation          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚          Neo4j Execution            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Natural Language Answer          â”‚
                    â”‚    + Explainable WHY reasoning      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SANTOS Duplicate Detection Algorithm

The cross-source duplicate detection adapts SANTOS for metadata-only comparison:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SANTOS Score Calculation                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   final_score = (0.40 Ã— semantic_score)    â† Column embeddings  â”‚
â”‚               + (0.25 Ã— schema_score)      â† Type matching      â”‚
â”‚               + (0.20 Ã— statistical_score) â† Row/col counts     â”‚
â”‚               + (0.15 Ã— relationship_score)â† FK patterns        â”‚
â”‚                                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Confidence Thresholds:                                          â”‚
â”‚  â€¢ High:   score â‰¥ 0.70                                         â”‚
â”‚  â€¢ Medium: score â‰¥ 0.30                                         â”‚
â”‚  â€¢ Low:    score < 0.30                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Research Questions & Results

### RQ1: Does GraphRAG outperform embeddings-only RAG?

**Hypothesis**: Hybrid retrieval (graph + vectors) improves accuracy by â‰¥25%

**Result**: âœ… **Yes** â€” Smart GraphRAG achieves 60% vs embeddings-only 50% (+20% improvement, p=0.114)

| Metric | Smart GraphRAG | Embeddings-Only | Improvement |
|--------|----------------|-----------------|-------------|
| Success@1 | 60.0% | 50.0% | +10pp |
| Success@3 | 78.3% | 71.7% | +6.6pp |
| MRR | 0.695 | 0.643 | +8.1% |

### RQ2: How much lineage can be inferred from Snowflake?

**Hypothesis**: Automated extraction achieves F1 â‰¥ 0.85 vs manual labeling

**Result**: âœ… **100% F1 score** â€” All 6 DERIVES_FROM edges correctly extracted

| Source | Target | Type | Confidence |
|--------|--------|------|------------|
| OLIST_MARKETING.CLIENT_DATA | OLIST_SALES.CUSTOMERS | CTAS | 100% |
| OLIST_ANALYTICS.CUSTOMER_MASTER | OLIST_SALES.CUSTOMERS | TRANSFORM | 85% |
| OLIST_MARKETING.SALES_ORDERS | OLIST_SALES.ORDERS | CTAS | 100% |
| OLIST_ANALYTICS.PURCHASE_HISTORY | OLIST_SALES.ORDERS | TRANSFORM | 62% |
| OLIST_MARKETING.PRODUCT_CATALOG | OLIST_SALES.PRODUCTS | CTAS | 100% |
| customer_feedback | sales_transactions | FOREIGN_KEY | 100% |

### RQ3: What SHACL constraints optimize governance?

**Hypothesis**: Moderate constraints achieve 95% coverage with <5% false positives

**Result**: âœ… **10 SHACL shapes** with 3 severity levels, <1s execution time

| Shape | Scope | Severity | Description |
|-------|-------|----------|-------------|
| TableOwnership | snowflake | critical | Every table must have owner |
| ColumnDataType | snowflake | warning | Columns must have data_type |
| LineageCompleteness | snowflake | info | Derived tables need lineage |
| DuplicateConfidence | snowflake | warning | Duplicates need confidence |
| FederatedTableSource | federated | critical | Must specify source |
| DatabricksOwnership | databricks | warning | Tables need owner |
| SensitivityClassification | databricks | info | PII columns need sensitivity |
| CrossSourceScore | cross-source | warning | SIMILAR_TO needs score |
| CrossSourceConfidence | cross-source | info | SIMILAR_TO needs confidence |
| PIIDetectionShape | databricks | warning | PII columns need High/Critical |

### RQ4: Do rules beat ML on small datasets?

**Hypothesis**: Rule-based routing matches or exceeds ML performance

**Result**: âœ… **Yes** â€” Rules achieve 60% vs XGBoost 53.3% (+6.7pp)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Routing Accuracy Comparison                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Smart (Rules)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  60.0%  â”‚
â”‚  Learned (XGB)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        53.3%  â”‚
â”‚  Embeddings-Only  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          50.0%  â”‚
â”‚  Keyword Search   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  40.0%  â”‚
â”‚  Graph-Only       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        30.0%  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RQ5: Does symbolic+neural routing beat pure neural?

**Hypothesis**: Hybrid approach outperforms pure neural classification

**Result**: âœ… **Yes** â€” 60% vs 14% (+46pp improvement)

### RQ6: Can SANTOS generalize across heterogeneous sources?

**Hypothesis**: Cross-platform detection maintains accuracy

**Result**: âœ… **Yes** â€” 16 cross-source SIMILAR_TO matches detected

| Databricks Table | Best Snowflake Match | Score | Confidence |
|------------------|---------------------|-------|------------|
| customer_feedback | OLIST_MARKETING.SALES_ORDERS | 36.0% | medium |
| customer_feedback | OLIST_SALES.ORDERS | 36.0% | medium |
| sales_transactions | OLIST_MARKETING.PRODUCT_CATALOG | 35.4% | medium |
| sales_transactions | OLIST_SALES.PRODUCTS | 35.4% | medium |
| sales_transactions | OLIST_SALES.ORDERS | 34.8% | medium |

---

## ğŸ›  Technology Stack

### Core Components

| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Knowledge Graph** | Neo4j | 5.x | Store metadata relationships, enable graph traversal |
| **Vector Database** | Milvus | 2.x | Semantic search with 384-dim embeddings |
| **LLM** | Ollama (llama3.1) | 7B | Natural language understanding, Cypher generation |
| **Data Warehouse** | Snowflake | - | Primary metadata source (INFORMATION_SCHEMA) |
| **Data Lakehouse** | Databricks | - | Secondary source (Unity Catalog) |
| **Embeddings** | sentence-transformers | all-MiniLM-L6-v2 | 384-dimensional semantic vectors |
| **Text-to-Cypher** | LangChain | 0.1.x | Prompt templates, few-shot examples |
| **ML Framework** | XGBoost | 1.7.x | Learned query routing baseline |
| **UI Framework** | Gradio | 4.x | 10-tab interactive demo interface |
| **Language** | Python | 3.10+ | Primary implementation language |

### Python Dependencies

```
# Core
neo4j>=5.0.0
pymilvus>=2.3.0
langchain>=0.1.0
sentence-transformers>=2.2.0

# Data Sources
snowflake-connector-python>=3.0.0
databricks-sql-connector>=2.0.0

# ML
xgboost>=1.7.0
scikit-learn>=1.0.0
numpy>=1.24.0
pandas>=2.0.0

# UI
gradio>=4.0.0

# Utilities
python-dotenv>=1.0.0
requests>=2.28.0
```

---

## ğŸ“¦ Installation

### Prerequisites

- **Python 3.10+** â€” [Download](https://www.python.org/downloads/)
- **Docker & Docker Compose** â€” [Install Docker](https://docs.docker.com/get-docker/)
- **Neo4j** â€” Desktop or Docker
- **Ollama** â€” [Install Ollama](https://ollama.ai/)
- **Snowflake Account** â€” With INFORMATION_SCHEMA access
- **Databricks Workspace** â€” (Optional) With Unity Catalog

### Step 1: Clone Repository

```bash
git clone https://github.com/yourusername/nexus-graphrag.git
cd nexus-graphrag
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python -m venv venv

# Activate (Linux/Mac)
source venv/bin/activate

# Activate (Windows)
venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### Step 4: Start Infrastructure Services

#### Neo4j (Option A: Docker)
```bash
docker run -d \
  --name neo4j \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/your_password \
  -e NEO4J_PLUGINS='["apoc"]' \
  neo4j:5
```

#### Neo4j (Option B: Desktop)
Download [Neo4j Desktop](https://neo4j.com/download/) and create a new database.

#### Milvus (Vector Database)
```bash
# Start Milvus standalone with Docker Compose
docker-compose up -d

# Verify Milvus is running
curl http://localhost:9091/healthz
```

#### Ollama (LLM)
```bash
# Start Ollama server
ollama serve

# In another terminal, pull the model
ollama pull llama3.1

# Verify model is available
curl http://localhost:11434/api/tags
```

### Step 5: Verify Services

```bash
# Check all services are running
echo "Checking Neo4j..."
curl -s http://localhost:7474 > /dev/null && echo "âœ… Neo4j OK" || echo "âŒ Neo4j FAILED"

echo "Checking Milvus..."
curl -s http://localhost:9091/healthz > /dev/null && echo "âœ… Milvus OK" || echo "âŒ Milvus FAILED"

echo "Checking Ollama..."
curl -s http://localhost:11434/api/tags > /dev/null && echo "âœ… Ollama OK" || echo "âŒ Ollama FAILED"
```

---

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the project root:

```bash
# ===========================================
# SNOWFLAKE CONFIGURATION
# ===========================================
SNOWFLAKE_ACCOUNT=your_account.region.cloud
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_DATABASE=TRAINING_DB
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_SCHEMA=OLIST_SALES
SNOWFLAKE_ROLE=ACCOUNTADMIN

# ===========================================
# NEO4J CONFIGURATION
# ===========================================
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password

# ===========================================
# DATABRICKS CONFIGURATION (Optional)
# ===========================================
DATABRICKS_HOST=https://dbc-xxxxx.cloud.databricks.com/
DATABRICKS_TOKEN=dapixxxxxxxxxxxxxxxxxxxxxxxxxxxxx
DATABRICKS_WAREHOUSE_ID=xxxxxxxxxxxxx
DATABRICKS_CATALOG=workspace
DATABRICKS_SCHEMA=sample_data

# ===========================================
# MILVUS CONFIGURATION
# ===========================================
MILVUS_HOST=localhost
MILVUS_PORT=19530

# ===========================================
# OLLAMA CONFIGURATION
# ===========================================
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1

# ===========================================
# APPLICATION SETTINGS
# ===========================================
LOG_LEVEL=INFO
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384
```

### Docker Compose Configuration

```yaml
# docker-compose.yaml
version: '3.8'

services:
  milvus-etcd:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
    volumes:
      - etcd_data:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd

  milvus-minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - minio_data:/minio_data
    command: minio server /minio_data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  milvus-standalone:
    image: milvusdb/milvus:v2.3.3
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: milvus-etcd:2379
      MINIO_ADDRESS: milvus-minio:9000
    volumes:
      - milvus_data:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - milvus-etcd
      - milvus-minio

volumes:
  etcd_data:
  minio_data:
  milvus_data:
```

---

## ğŸ”¨ Building the Knowledge Graph

### Complete Build Pipeline

```bash
# ===========================================
# STEP 1: Upload Olist Data to Snowflake
# ===========================================
python scripts/olist_uploader.py

# Expected output:
# âœ… Uploaded 9 CSV files to Snowflake
# âœ… Created 3 schemas: OLIST_SALES, OLIST_MARKETING, OLIST_ANALYTICS
# âœ… Total rows: 1,400,000+

# ===========================================
# STEP 2: Build Snowflake Metadata KG
# ===========================================
python scripts/load_olist_to_kg.py

# Expected output:
# âœ… Created 13 OlistData nodes
# âœ… Created 82 OlistColumn nodes
# âœ… Created 82 HAS_COLUMN relationships
# âœ… Created 3 OLIST_DUPLICATE relationships

# ===========================================
# STEP 3: Load Sample Data (Customer, Order, Product)
# ===========================================
python scripts/load_json_to_kg.py

# Expected output:
# âœ… Created 100 Customer nodes
# âœ… Created 100 Order nodes
# âœ… Created 98 Product nodes
# âœ… Created 100 PLACED relationships

# ===========================================
# STEP 4: Extract Lineage from Query History
# ===========================================
python scripts/extract_lineage.py

# Expected output:
# âœ… Extracted 5 DERIVES_FROM edges from Snowflake
# âœ… Identified CTAS and TRANSFORM patterns

# ===========================================
# STEP 5: Build Federation (Databricks)
# ===========================================
python -c "
from src.federation import build_federated_graph
build_federated_graph()
"

# Expected output:
# âœ… Created 2 DataSource nodes
# âœ… Created 15 FederatedTable nodes
# âœ… Created 25 FederatedColumn nodes

# ===========================================
# STEP 6: Run Cross-Source Detection
# ===========================================
python -m src.federation.cross_source_duplicate_detector

# Expected output:
# âœ… Created 12 SIMILAR_TO relationships
# âœ… Detected cross-platform matches

# ===========================================
# STEP 7: Index Vectors in Milvus
# ===========================================
python src/graphrag/vector_indexer.py --rebuild

# Expected output:
# âœ… Indexed 15 tables in Milvus
# âœ… Created 384-dimensional embeddings
# âœ… Built HNSW index
```

### Verify Knowledge Graph

```bash
# Check node counts
python -c "
from neo4j import GraphDatabase
driver = GraphDatabase.driver('bolt://localhost:7687', auth=('neo4j', 'password'))
with driver.session() as session:
    result = session.run('''
        MATCH (n)
        RETURN labels(n)[0] as label, count(*) as count
        ORDER BY count DESC
    ''')
    for record in result:
        print(f'{record[\"label\"]}: {record[\"count\"]}')
"

# Expected output:
# Customer: 100
# Order: 100
# Product: 98
# OlistColumn: 82
# FederatedColumn: 25
# FederatedTable: 15
# OlistData: 13
# DataSource: 2
```

---

## ğŸš€ Running the Demo

### Launch Gradio Interface

```bash
python demo_gradio.py

# Output:
# Running on local URL:  http://localhost:7860
# Running on public URL: https://xxxxx.gradio.live (if share=True)
```

### Demo Interface Tabs

| Tab | Name | Function |
|-----|------|----------|
| 1 | ğŸŒŸ Unified Search | Query metadata + sample data + Databricks |
| 2 | ğŸ”— Lineage Explorer | Visualize upstream/downstream data flow |
| 3 | âš”ï¸ Compare Engines | Side-by-side Smart vs Learned comparison |
| 4 | ğŸ”„ Duplicates | Within-Snowflake + Cross-Source detection |
| 5 | ğŸ“Š Performance | Benchmark metrics visualization |
| 6 | âš™ï¸ System | Knowledge graph statistics |
| 7 | ğŸ›¡ Governance | SHACL validation interface |
| 8 | ğŸŒ Federation | Multi-source overview |
| 9 | ğŸ“ˆ Metrics | Detailed evaluation results |
| 10 | â“ Help | Usage instructions |

### Quick Test

```bash
# Test the GraphRAG engine directly
python -c "
from src.graphrag.unified_llm_graphrag import UnifiedLLMGraphRAG

engine = UnifiedLLMGraphRAG()

# Test sample data query
result = engine.query('How many customers from SÃ£o Paulo?')
print('Sample Query:', result['nl_answer'])

# Test metadata query
result = engine.query('Which tables have the most rows?')
print('Metadata Query:', result['nl_answer'])

# Test cross-source query
result = engine.query('Which Databricks tables are similar to Snowflake?')
print('Cross-Source:', result['nl_answer'])

engine.close()
"
```

---

## ğŸ“ Project Structure

```
nexus-data-catalog/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          # This file
â”œâ”€â”€ ğŸ“„ requirements.txt                   # Python dependencies
â”œâ”€â”€ ğŸ“„ docker-compose.yaml                # Milvus services
â”œâ”€â”€ ğŸ“„ .env.example                       # Environment template
â”œâ”€â”€ ğŸ“„ .gitignore                         # Git ignore rules
â”‚
â”œâ”€â”€ ğŸš€ main.py                            # Basic ETL pipeline
â”œâ”€â”€ ğŸ–¥ï¸ demo_gradio.py                     # 10-tab professional UI
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”œâ”€â”€ ğŸ“‚ olist_data/                    # Source CSV files
â”‚   â”‚   â”œâ”€â”€ olist_customers.csv
â”‚   â”‚   â”œâ”€â”€ olist_orders.csv
â”‚   â”‚   â”œâ”€â”€ olist_products.csv
â”‚   â”‚   â””â”€â”€ ... (9 files total)
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ evaluation/
â”‚   â”‚   â”œâ”€â”€ benchmark_questions.json      # 60 test questions
â”‚   â”‚   â””â”€â”€ comparative_results.json      # Evaluation output
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ training/
â”‚       â””â”€â”€ performance_based_labels.json # XGBoost training data
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ connectors/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ snowflake_connector.py     # Snowflake connection
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ extractors/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ metadata_extractor.py      # INFORMATION_SCHEMA extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ knowledge_graph/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ kg_builder.py              # Base KG builder
â”‚   â”‚   â””â”€â”€ ğŸ“„ olist_kg_builder.py        # Olist-specific construction
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ graphrag/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ smart_graphrag_engine.py   # Rule-based routing (60%)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ learned_graphrag_engine.py # XGBoost routing (53.3%)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ llm_enhanced_smart_graphrag.py  # LLM answer generation
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ explainable_graphrag.py    # WHY explanations
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ unified_llm_graphrag.py    # Master router
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ langchain_graphrag.py      # Text-to-Cypher
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ query_features.py          # Feature extraction
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ vector_indexer.py          # Milvus indexing
â”‚   â”‚   â””â”€â”€ ğŸ“„ few_shot_examples.py       # 38 Cypher examples
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ lineage/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ snowflake_lineage_extractor.py  # QUERY_HISTORY parsing
â”‚   â”‚   â””â”€â”€ ğŸ“„ lineage_graph_builder.py   # DERIVES_FROM edges
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ federation/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ databricks_metadata_extractor.py  # Unity Catalog
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ federated_kg_builder.py    # Multi-source KG
â”‚   â”‚   â””â”€â”€ ğŸ“„ cross_source_duplicate_detector.py  # SANTOS cross-source
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ governance/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ shacl_validator.py         # SHACL-inspired validation
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ evaluation/
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â””â”€â”€ ğŸ“„ baseline_systems.py        # Keyword, embeddings baselines
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ ğŸ“„ olist_uploader.py              # Upload CSVs to Snowflake
â”‚   â”œâ”€â”€ ğŸ“„ load_olist_to_kg.py            # Build Neo4j from Snowflake
â”‚   â”œâ”€â”€ ğŸ“„ load_json_to_kg.py             # Load sample data
â”‚   â”œâ”€â”€ ğŸ“„ run_comparative_evaluation.py  # Run 60-question benchmark
â”‚   â”œâ”€â”€ ğŸ“„ create_performance_labels.py   # Generate training labels
â”‚   â”œâ”€â”€ ğŸ“„ extract_lineage.py             # Lineage extraction pipeline
â”‚   â””â”€â”€ ğŸ“„ train_route_classifier.py      # Train XGBoost model
â”‚
â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”œâ”€â”€ ğŸ“„ route_classifier.pkl           # Trained XGBoost model
â”‚   â”œâ”€â”€ ğŸ“„ label_encoder.pkl              # Label encoding
â”‚   â””â”€â”€ ğŸ“„ feature_names.json             # Feature list
â”‚
â””â”€â”€ ğŸ“‚ thesis/                            # Thesis documents
    â””â”€â”€ ğŸ“‚ chapters/
```

---

## ğŸ—„ Knowledge Graph Schema

### Node Types (4-Layer Structure)

#### Layer 1: Sample Data
```cypher
(:Customer {
  customer_id: STRING,     // Unique identifier
  city: STRING,            // Customer city
  state: STRING            // Customer state (2-letter code)
})

(:Order {
  order_id: STRING,        // Unique identifier
  customer_id: STRING,     // FK to Customer
  status: STRING           // delivered, shipped, canceled, etc.
})

(:Product {
  product_id: STRING,      // Unique identifier
  category: STRING,        // English category name
  category_pt: STRING      // Portuguese category name
})
```

#### Layer 2: Snowflake Metadata
```cypher
(:OlistData {
  name: STRING,            // Table name
  schema: STRING,          // Schema name (OLIST_SALES, etc.)
  database: STRING,        // Database name (TRAINING_DB)
  row_count: INTEGER,      // Number of rows
  column_count: INTEGER,   // Number of columns
  fingerprint: STRING,     // Metadata hash for duplicate detection
  owner: STRING,           // Owning team
  created_at: DATETIME     // Creation timestamp
})

(:OlistColumn {
  name: STRING,            // Column name
  data_type: STRING,       // JSON format type info
  ordinal_position: INTEGER,
  is_nullable: BOOLEAN,
  character_maximum_length: INTEGER
})
```

#### Layer 3: Databricks Metadata
```cypher
(:FederatedTable {
  full_name: STRING,       // catalog.schema.table
  table_name: STRING,      // Short name
  source: STRING,          // 'databricks' or 'snowflake'
  schema: STRING,          // Schema name
  catalog: STRING,         // Catalog name
  row_count: INTEGER,
  column_count: INTEGER,
  owner: STRING,
  column_signature: STRING,  // Sorted column names hash
  type_signature: STRING,    // Sorted data types hash
  created_at: DATETIME
})

(:FederatedColumn {
  name: STRING,
  data_type: STRING,       // With ColumnTypeName. prefix
  position: INTEGER,
  sensitivity: STRING,     // None, Low, Medium, High, Critical
  nullable: BOOLEAN,
  comment: STRING
})

(:DataSource {
  name: STRING,            // 'snowflake' or 'databricks'
  type: STRING             // Platform type
})
```

### Relationship Types

| Relationship | From | To | Properties | Description |
|--------------|------|-----|------------|-------------|
| `PLACED` | Customer | Order | - | Customer placed order |
| `CONTAINS` | Order | Product | - | Order contains product |
| `HAS_COLUMN` | OlistData/FederatedTable | Column | - | Table has column |
| `OLIST_DUPLICATE` | OlistData | OlistData | confidence, match_type, semantic_score | Within-Snowflake duplicate |
| `DERIVES_FROM` | Table | Table | lineage_type, confidence, query_id, discovered_at | Data lineage |
| `SIMILAR_TO` | FederatedTable | OlistData | score, confidence, semantic_score, schema_score, statistical_score, relationship_score, matching_columns | Cross-source match |
| `FROM_SOURCE` | FederatedTable | DataSource | - | Table belongs to source |
| `MIRRORS` | FederatedTable | OlistData | - | Snowflake FederatedTable mirrors OlistData |
| `FOREIGN_KEY` | OlistData | OlistData | - | Foreign key relationship |

### Sample Cypher Queries

```cypher
// Find all tables with their column counts
MATCH (t:OlistData)-[:HAS_COLUMN]->(c:OlistColumn)
RETURN t.name as table, t.schema as schema, count(c) as columns
ORDER BY columns DESC

// Find duplicate tables within Snowflake
MATCH (t1:OlistData)-[r:OLIST_DUPLICATE]->(t2:OlistData)
RETURN t1.name, t2.name, r.confidence, r.match_type

// Find cross-source matches
MATCH (db:FederatedTable)-[r:SIMILAR_TO]->(sf:OlistData)
WHERE db.source = 'databricks'
RETURN db.table_name, sf.name, r.score, r.confidence
ORDER BY r.score DESC

// Trace lineage upstream
MATCH path = (target:OlistData {name: 'CLIENT_DATA'})-[:DERIVES_FROM*1..3]->(source:OlistData)
RETURN path

// Find tables by owner
MATCH (t:OlistData)
WHERE t.owner = 'data_engineering_team'
RETURN t.name, t.schema, t.row_count
```

---

## ğŸ“Š Datasets

### Snowflake: Olist Brazilian E-Commerce

The [Olist dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce) is a Brazilian e-commerce public dataset containing ~100k orders from 2016-2018.

| Schema | Table | Rows | Columns | Owner |
|--------|-------|------|---------|-------|
| OLIST_SALES | CUSTOMERS | 99,441 | 5 | data_engineering_team |
| OLIST_SALES | ORDERS | 99,441 | 8 | data_engineering_team |
| OLIST_SALES | PRODUCTS | 32,951 | 9 | data_engineering_team |
| OLIST_SALES | SELLERS | 3,095 | 4 | data_engineering_team |
| OLIST_SALES | GEOLOCATION | 1,000,163 | 5 | data_engineering_team |
| OLIST_SALES | ORDER_ITEMS | 112,650 | 7 | data_engineering_team |
| OLIST_SALES | ORDER_PAYMENTS | 103,886 | 5 | data_engineering_team |
| OLIST_SALES | ORDER_REVIEWS | 99,224 | 7 | data_engineering_team |
| OLIST_MARKETING | CLIENT_DATA | 99,441 | 5 | marketing_analytics_team |
| OLIST_MARKETING | SALES_ORDERS | 99,441 | 8 | marketing_analytics_team |
| OLIST_MARKETING | PRODUCT_CATALOG | 32,951 | 9 | marketing_analytics_team |
| OLIST_ANALYTICS | CUSTOMER_MASTER | 99,441 | 5 | business_intelligence_team |
| OLIST_ANALYTICS | PURCHASE_HISTORY | 99,441 | 8 | business_intelligence_team |

**Total**: 13 tables, ~1.4M rows, 82 columns

### Databricks: Unity Catalog Sample Data

| Table | Rows | Columns | Owner |
|-------|------|---------|-------|
| sales_transactions | 150 | 13 | sales_team |
| customer_feedback | 100 | 12 | customer_experience_team |

### Neo4j Sample Data (Layer 2)

| Node Type | Count | Source |
|-----------|-------|--------|
| Customer | 100 | Sampled from Olist |
| Order | 100 | Sampled from Olist |
| Product | 98 | Sampled from Olist |

---

## ğŸ” Query Types & Examples

### 1. Sample Data Queries (â†’ Customer, Order, Product)

```
"How many customers from SÃ£o Paulo?"
"Show me delivered orders"
"Which customers bought furniture?"
"List products in electronics category"
"Which customers placed the most orders?"
```

### 2. Metadata Queries (â†’ OlistData, OlistColumn)

```
"Which tables have the most rows?"
"Find tables in OLIST_SALES schema"
"Show duplicate tables"
"What does CLIENT_DATA derive from?"
"Show columns in CUSTOMERS table"
"Who owns the ORDERS table?"
```

### 3. Databricks Queries (â†’ FederatedTable, FederatedColumn)

```
"List all Databricks columns"
"What columns are in sales_transactions?"
"Which columns have high sensitivity?"
"Who owns customer_feedback?"
"Show me Databricks tables"
```

### 4. Cross-Source Queries (â†’ SIMILAR_TO relationships)

```
"Find cross-source matches"
"Which Databricks tables are similar to Snowflake?"
"What Snowflake tables match sales_transactions?"
"Why is customer_feedback similar to ORDERS?"
"Explain the match between sales_transactions and PRODUCTS"
```

### 5. Governance Queries

```
"Which tables have PII columns?"
"Show tables without owners"
"Find high sensitivity columns"
"Validate governance constraints"
```

---

## ğŸ“ˆ Evaluation Methodology

### Benchmark Dataset

- **60 questions** across all query types
- **Ground truth** manually labeled by domain expert
- **Categories**: sample_data (15), metadata (20), databricks (10), cross_source (8), governance (7)

### Metrics

| Metric | Description |
|--------|-------------|
| **Success@1** | Correct answer in top result |
| **Success@3** | Correct answer in top 3 results |
| **MRR** | Mean Reciprocal Rank |
| **Intent Accuracy** | Query type classification accuracy |

### Baseline Systems

1. **Keyword Search**: TF-IDF based retrieval
2. **Embeddings-Only**: Vector similarity without graph
3. **Graph-Only**: Pure Cypher traversal
4. **Learned (XGBoost)**: ML-based query routing

### Statistical Testing

- **McNemar's Test**: Compare paired accuracy
- **Bootstrap CI**: 95% confidence intervals
- **Effect Size**: Cohen's d for practical significance

### Results Summary

| System | Success@1 | Success@3 | MRR | p-value vs Smart |
|--------|-----------|-----------|-----|------------------|
| **Smart GraphRAG** | **60.0%** | **78.3%** | **0.695** | - |
| Learned (XGBoost) | 53.3% | 68.3% | 0.603 | p=0.180 |
| Embeddings-Only | 50.0% | 71.7% | 0.643 | p=0.114 |
| Keyword Search | 40.0% | 46.7% | 0.432 | p=0.228 |
| Graph-Only | 30.0% | 36.7% | 0.333 | p=0.027 âœ… |

**Key Finding**: Smart GraphRAG significantly outperforms Graph-Only (p=0.027) and shows substantial improvement over Embeddings-Only (+20%).

---

## ğŸ“š API Reference

### UnifiedLLMGraphRAG

Main entry point for all queries.

```python
from src.graphrag.unified_llm_graphrag import UnifiedLLMGraphRAG

engine = UnifiedLLMGraphRAG()

# Basic query
result = engine.query("Which tables have the most rows?")
print(result['nl_answer'])
print(result['query_type'])
print(result['cypher_query'])

# Query with explanation
result = engine.query("Why is customer_feedback similar to ORDERS?")
print(result['explanation_type'])

# Close connection
engine.close()
```

#### Methods

| Method | Parameters | Returns | Description |
|--------|------------|---------|-------------|
| `query(question, top_k=5)` | question: str, top_k: int | dict | Execute natural language query |
| `classify_query_intent(question)` | question: str | str | Classify query type |
| `explain_match(db_table, sf_table)` | table names | str | Explain cross-source match |
| `close()` | - | - | Close connections |

#### Return Dictionary

```python
{
    'nl_answer': str,        # Natural language answer
    'query_type': str,       # classified intent
    'cypher_query': str,     # Generated Cypher
    'raw_results': list,     # Neo4j results
    'explanation_type': str, # Type of explanation (if applicable)
    'latency_ms': float      # Query latency
}
```

### ExplainableGraphRAG

Generates human-readable explanations for cross-source matches.

```python
from src.graphrag.explainable_graphrag import ExplainableGraphRAG

engine = ExplainableGraphRAG()

# Explain a specific match
explanation = engine.explain_match('customer_feedback', 'SALES_ORDERS')
print(explanation)
# Output: "The Databricks table customer_feedback is most similar to 
#          Snowflake's OLIST_MARKETING.SALES_ORDERS with a 36.0% match 
#          score. This similarity is driven by..."

engine.close()
```

### SHACLValidator

Validates knowledge graph against governance constraints.

```python
from src.governance.shacl_validator import SHACLValidator

validator = SHACLValidator()

# Run all validations
results = validator.validate_all()

# Check specific constraint
ownership_violations = validator.validate_table_ownership()

# Get violation summary
summary = validator.get_violation_summary()
print(f"Critical: {summary['critical']}")
print(f"Warning: {summary['warning']}")
print(f"Info: {summary['info']}")
```

---

## âš ï¸ Known Limitations

1. **Small Databricks Dataset**: Only 2 tables (250 rows) â€” proof of concept only

2. **Cross-Source Threshold**: Matches are 30-35%, below "high confidence" threshold. This is expected for metadata-only comparison without value intersection.

3. **No Value Comparison**: SANTOS adapted for metadata-only (privacy by design). Original SANTOS uses value overlap which would yield higher scores.

4. **LLM Latency**: Ollama local inference adds ~2-5 seconds per query. Consider GPU acceleration for production.

5. **Single LLM Model**: Uses llama3.1 only. No comparison with GPT-4, Claude, or other models.

6. **Manual Ground Truth**: 60 questions manually labeled. May not cover all edge cases.

7. **Explanation Quality**: WHY explanations not formally evaluated (qualitative only).

8. **Scale Limitations**: Tested on 13 Snowflake + 2 Databricks tables. Enterprise scale (1000+ tables) not validated.

---

## ğŸ”® Future Work

### Short-Term (Next Release)

- [ ] Additional data sources: PostgreSQL, BigQuery, S3/Delta Lake
- [ ] Multi-model comparison: OpenAI, Claude, Mistral
- [ ] Graph visualization tab in Gradio UI
- [ ] Query history and favorites

### Medium-Term (Research Extensions)

- [ ] **Value-based SANTOS**: If privacy allows, implement original value intersection
- [ ] **Graph Neural Networks**: Replace rule-based with learned cross-source matching
- [ ] **Active Learning**: User feedback to tune similarity thresholds
- [ ] **Real-time Lineage**: Stream processing for live lineage updates

### Long-Term (Publication Opportunities)

- [ ] Formal explanation evaluation with user study
- [ ] Benchmark dataset contribution for metadata QA
- [ ] Theoretical analysis of graph-based duplicate detection complexity
- [ ] Federated GraphRAG across organizations with privacy preservation

---

## ğŸ“š References

1. **SANTOS**: Khatiwada, A., et al. "SANTOS: Relationship-based Semantic Table Union Search." *SIGMOD 2023*. [Paper](https://dl.acm.org/doi/10.1145/3588689)

2. **GraphRAG**: Microsoft Research. "From Local to Global: A Graph RAG Approach to Query-Focused Summarization." 2024. [Paper](https://arxiv.org/abs/2404.16130)

3. **SHACL**: W3C. "Shapes Constraint Language (SHACL)." 2017. [Specification](https://www.w3.org/TR/shacl/)

4. **Olist Dataset**: Olist. "Brazilian E-Commerce Public Dataset." *Kaggle*. [Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

5. **LangChain**: Chase, H., et al. "LangChain: Building applications with LLMs through composability." 2022. [Documentation](https://langchain.com/)

6. **Neo4j**: Neo4j, Inc. "Neo4j Graph Database." [Documentation](https://neo4j.com/docs/)

7. **Milvus**: Zilliz. "Milvus: A Purpose-Built Vector Database." [Documentation](https://milvus.io/docs)

8. **Ollama**: Ollama. "Get up and running with large language models locally." [Website](https://ollama.ai/)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2025 Pranav Kharat

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ™ Acknowledgments

- **Course Instructor**: Professor and course staff of DAMG7374 for guidance throughout the project
- **SANTOS Authors**: Khatiwada et al. for the foundational research on relationship-based semantic table matching
- **Olist**: For providing the comprehensive Brazilian e-commerce dataset
- **Open Source Community**: Neo4j, Milvus, LangChain, Ollama, and Gradio teams

---

<p align="center">
  <b>Built with â¤ï¸ at Northeastern University</b><br>
  <i>DAMG7374 - LLM with Knowledge Graph DB</i><br>
  <i>Fall 2025</i>
</p>
```